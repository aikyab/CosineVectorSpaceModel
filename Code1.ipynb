{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ca40854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aikyab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Id's and document Id's in pairs (of descending order) :\n",
      "\n",
      "For top 10 documents, we have :\n",
      "\n",
      "Queries\t\tPrecision\tRecall\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Query 1 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 2 \t 0.2 \t\t 0.13333333333333333 \n",
      "\n",
      "Query 3 \t 0.3 \t\t 0.2 \n",
      "\n",
      "Query 4 \t 0.1 \t\t 0.05555555555555555 \n",
      "\n",
      "Query 5 \t 0.2 \t\t 0.10526315789473684 \n",
      "\n",
      "Query 6 \t 0.2 \t\t 0.1111111111111111 \n",
      "\n",
      "Query 7 \t 0.4 \t\t 0.4444444444444444 \n",
      "\n",
      "Query 8 \t 0.1 \t\t 0.25 \n",
      "\n",
      "Query 9 \t 0.1 \t\t 0.125 \n",
      "\n",
      "Query 10 \t 0.2 \t\t 0.08333333333333333 \n",
      "\n",
      "Average Precision (across all 10 queries):  0.18 \n",
      "\n",
      "Average Recall (across all 10 queries):  0.15080409356725147 \n",
      "\n",
      "\n",
      "For top 50 documents, we have :\n",
      "\n",
      "Queries\t\tPrecision\tRecall\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Query 1 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 2 \t 0.12 \t\t 0.4 \n",
      "\n",
      "Query 3 \t 0.12 \t\t 0.4 \n",
      "\n",
      "Query 4 \t 0.04 \t\t 0.1111111111111111 \n",
      "\n",
      "Query 5 \t 0.18 \t\t 0.47368421052631576 \n",
      "\n",
      "Query 6 \t 0.1 \t\t 0.2777777777777778 \n",
      "\n",
      "Query 7 \t 0.1 \t\t 0.5555555555555556 \n",
      "\n",
      "Query 8 \t 0.06 \t\t 0.75 \n",
      "\n",
      "Query 9 \t 0.12 \t\t 0.75 \n",
      "\n",
      "Query 10 \t 0.06 \t\t 0.125 \n",
      "\n",
      "Average Precision (across all 10 queries):  0.09 \n",
      "\n",
      "Average Recall (across all 10 queries):  0.38431286549707605 \n",
      "\n",
      "\n",
      "For top 100 documents, we have :\n",
      "\n",
      "Queries\t\tPrecision\tRecall\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Query 1 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 2 \t 0.09 \t\t 0.6 \n",
      "\n",
      "Query 3 \t 0.09 \t\t 0.6 \n",
      "\n",
      "Query 4 \t 0.05 \t\t 0.2777777777777778 \n",
      "\n",
      "Query 5 \t 0.12 \t\t 0.631578947368421 \n",
      "\n",
      "Query 6 \t 0.09 \t\t 0.5 \n",
      "\n",
      "Query 7 \t 0.07 \t\t 0.7777777777777778 \n",
      "\n",
      "Query 8 \t 0.03 \t\t 0.75 \n",
      "\n",
      "Query 9 \t 0.07 \t\t 0.875 \n",
      "\n",
      "Query 10 \t 0.03 \t\t 0.125 \n",
      "\n",
      "Average Precision (across all 10 queries):  0.06400000000000002 \n",
      "\n",
      "Average Recall (across all 10 queries):  0.5137134502923977 \n",
      "\n",
      "\n",
      "For top 500 documents, we have :\n",
      "\n",
      "Queries\t\tPrecision\tRecall\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Query 1 \t 0.002 \t\t 1.0 \n",
      "\n",
      "Query 2 \t 0.03 \t\t 1.0 \n",
      "\n",
      "Query 3 \t 0.03 \t\t 1.0 \n",
      "\n",
      "Query 4 \t 0.034 \t\t 0.9444444444444444 \n",
      "\n",
      "Query 5 \t 0.038 \t\t 1.0 \n",
      "\n",
      "Query 6 \t 0.034 \t\t 0.9444444444444444 \n",
      "\n",
      "Query 7 \t 0.018 \t\t 1.0 \n",
      "\n",
      "Query 8 \t 0.008 \t\t 1.0 \n",
      "\n",
      "Query 9 \t 0.016 \t\t 1.0 \n",
      "\n",
      "Query 10 \t 0.032 \t\t 0.6666666666666666 \n",
      "\n",
      "Average Precision (across all 10 queries):  0.024200000000000003 \n",
      "\n",
      "Average Recall (across all 10 queries):  0.9555555555555555 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Name: Aikya Banerjee \n",
    "#UIN : 675064035 \n",
    "#Course : CS494 Information Retrieval and Web Search\n",
    "#HW2\n",
    "\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "my_path = os.getcwd()\n",
    "\n",
    "\n",
    "def stemmerStopEliminator(word_list):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    dict_stop_words = Counter(stop_words)\n",
    "    new_words = []\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    for word in word_list:\n",
    "        if dict_stop_words[word]==0:\n",
    "            new_word = ps.stem(word)\n",
    "            if dict_stop_words[new_word]==0:\n",
    "                new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def tokenize_documents():\n",
    "    doc_folder = my_path+\"/cranfieldDocs\"\n",
    "    os.chdir(doc_folder)\n",
    "    onlyfiles = [f for f in listdir(doc_folder) if isfile(join(doc_folder, f))]\n",
    "    text_corpus = {}\n",
    "    regex_EXP = re.compile(r'[^\\W\\d]+')\n",
    "    for path in onlyfiles:\n",
    "        with open(path,encoding=\"latin-1\") as f:\n",
    "            text = f.readlines()\n",
    "            flag = 0\n",
    "            matcher = re.search(\"[\\d]+\",f.name)\n",
    "            file_name = int(matcher.group())\n",
    "            string_title = \"\"\n",
    "            string_text = \"\"\n",
    "            for line in text:\n",
    "                if \"<TITLE>\" in line:\n",
    "                    flag = 1\n",
    "                    continue\n",
    "                if \"</TITLE>\" in line:\n",
    "                    flag = 0\n",
    "                if flag == 1:\n",
    "                    string_title+=line\n",
    "            flag = 0\n",
    "            for line in text:\n",
    "                if \"<TEXT>\" in line:\n",
    "                    flag = 1\n",
    "                    continue\n",
    "                if \"</TEXT>\" in line:\n",
    "                    flag = 0\n",
    "                if flag == 1:\n",
    "                    string_text+=line\n",
    "            \n",
    "            word_match=regex_EXP.finditer(string_title)\n",
    "            words = []\n",
    "            for i in word_match:\n",
    "                words.append(i.group())\n",
    "            word_match=regex_EXP.finditer(string_text)\n",
    "            for i in word_match:\n",
    "                words.append(i.group())\n",
    "            trimmed_words = []\n",
    "            for word in words:\n",
    "                if len(word)!=1 and len(word)!=2:\n",
    "                    trimmed_words.append(word.lower())\n",
    "            new_stemmed_words = stemmerStopEliminator(trimmed_words)\n",
    "            text_corpus[file_name]=new_stemmed_words\n",
    "    return text_corpus\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_queries():\n",
    "    os.chdir(my_path)\n",
    "    with open(\"queries.txt\",encoding=\"latin-1\") as f:\n",
    "        queries = {}\n",
    "        text = f.readlines()\n",
    "        counter = 1\n",
    "        for line in text:\n",
    "            words = []\n",
    "            regex_EXP = re.compile(r'[^\\W\\d]+')\n",
    "            word_match=regex_EXP.finditer(line)\n",
    "            \n",
    "            for i in word_match:\n",
    "                words.append(i.group())\n",
    "            trimmed_words = []\n",
    "            for word in words:\n",
    "                if len(word)!=1 and len(word)!=2:\n",
    "                    trimmed_words.append(word.lower())\n",
    "            new_stemmed_words = stemmerStopEliminator(trimmed_words)\n",
    "            queries[counter]=new_stemmed_words\n",
    "            counter+=1\n",
    "    return queries\n",
    "\n",
    "doc_collection = tokenize_documents()\n",
    "query_collection = tokenize_queries()\n",
    "\n",
    "\n",
    "\n",
    "def df_calc(doc_collection):\n",
    "    df = defaultdict(list)\n",
    "    for key,value in doc_collection.items():\n",
    "        for word in value:\n",
    "            if key not in df[word]:\n",
    "                df[word].append(key)\n",
    "    return df\n",
    "\n",
    "                \n",
    "df = df_calc(doc_collection)\n",
    "\n",
    "def cosine_similarity(top_n):\n",
    "    doc_length = {}\n",
    "    for key_i,value_i in doc_collection.items():\n",
    "        tf = Counter(value_i)\n",
    "        doc_l = 0\n",
    "        for key_j,value_j in tf.items():\n",
    "            div_l = len(doc_collection)/len(df[key_j])\n",
    "            idf_l = math.log(div_l,2)\n",
    "            weight = value_j*idf_l\n",
    "            doc_l += weight*weight\n",
    "        doc_length[key_i] = math.sqrt(doc_l)\n",
    "    cosine_sim = {}\n",
    "    for key_i,value_i in doc_collection.items():\n",
    "        for key_j,value_j in query_collection.items():\n",
    "            sumx = 0\n",
    "            for word in value_j:\n",
    "                if len(df[word])==0:\n",
    "                    continue\n",
    "                div = len(doc_collection)/len(df[word])\n",
    "                idf = math.log(div,2)\n",
    "                term_f = value_i.count(word)\n",
    "                sumx += term_f*idf*idf\n",
    "            if key_j not in cosine_sim:\n",
    "                cosine_sim[key_j] = [(key_i,sumx/doc_length[key_i])]\n",
    "            else:\n",
    "                cosine_sim[key_j].append((key_i,sumx/doc_length[key_i]))\n",
    "    def take_second(elem):\n",
    "        return elem[1]\n",
    "    for key,value in cosine_sim.items():\n",
    "        value.sort(key=take_second,reverse=True)\n",
    "        l1 = []\n",
    "        for val in value:\n",
    "            l1.append(val[0])\n",
    "        if top_n<0:\n",
    "            value[:] = l1\n",
    "        else:\n",
    "            value[:] = l1[:top_n]\n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "def process_rel():\n",
    "    os.chdir(my_path)\n",
    "    with open(\"relevance.txt\",\"r\") as f:\n",
    "        text = f.readlines()\n",
    "        rel = {}\n",
    "        for line in text:\n",
    "            line_x = re.findall(\"[\\d]+\",line)\n",
    "            if int(line_x[0]) not in rel:\n",
    "                rel[int(line_x[0])] = [int(line_x[1])]\n",
    "            else:\n",
    "                rel[int(line_x[0])].append(int(line_x[1]))\n",
    "    return rel\n",
    "            \n",
    "\n",
    "def common_docs(l1,l2):\n",
    "    counter = Counter(l2)\n",
    "    final = []\n",
    "    for i in l1:\n",
    "        if counter[i]>0:\n",
    "            final.append(counter)\n",
    "    return len(final)\n",
    "\n",
    "queries = process_rel()\n",
    "\n",
    "def calc_recall(top_n):\n",
    "    cos_sim = cosine_similarity(top_n)\n",
    "    recall = {}\n",
    "    for i in range(1,len(queries)+1):\n",
    "        l1 = queries[i]\n",
    "        l2 = cos_sim[i]\n",
    "        retrieved = common_docs(l1,l2)\n",
    "        total = len(queries[i])\n",
    "        recall[i] = retrieved/total\n",
    "    return recall\n",
    "\n",
    "def calc_precision(top_n):\n",
    "    cos_sim = cosine_similarity(top_n)\n",
    "    precision = {}\n",
    "    for i in range(1,len(queries)+1):\n",
    "        l1 = queries[i]\n",
    "        l2 = cos_sim[i]\n",
    "        retrieved = common_docs(l1,l2)\n",
    "        total = top_n\n",
    "        precision[i] = retrieved/total\n",
    "    return precision\n",
    "\n",
    "print(\"Query Id's and document Id's in pairs (of descending order) :\\n\")\n",
    "\n",
    "final_list = []\n",
    "\n",
    "def list_retrieved():\n",
    "    for key,value in cosine_similarity(-1).items():\n",
    "        for doc_id in value:\n",
    "            final_list.append((key,doc_id))\n",
    "    \n",
    "list_retrieved()\n",
    "        \n",
    "print(final_list,\"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "def precision_recall_output():\n",
    "    for i in [10,50,100,500]:\n",
    "        print(\"For top\",i,\"documents, we have :\\n\")\n",
    "        print(\"Queries\\t\\tPrecision\\tRecall\\n\")\n",
    "        print(\"----------------------------------------------\\n\")\n",
    "        prec = calc_precision(i)\n",
    "        recall = calc_recall(i)\n",
    "        for j in range(1,len(queries)+1):\n",
    "            print(\"Query\",j,\"\\t\",prec[j],\"\\t\\t\",recall[j],\"\\n\")\n",
    "        print(\"Average Precision (across all\",len(queries),\"queries): \",sum(prec.values())/len(prec),\"\\n\")\n",
    "        print(\"Average Recall (across all\",len(queries),\"queries): \",sum(recall.values())/len(recall),\"\\n\\n\")\n",
    "    \n",
    "precision_recall_output()\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c3ded66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Id's and document Id's in pairs (of descending order) :\n",
      "\n",
      "[(1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2), (4, 1), (4, 2), (5, 1), (5, 2), (6, 1), (6, 2), (7, 1), (7, 2), (8, 1), (8, 2), (9, 1), (9, 2), (10, 1), (10, 2)] \n",
      "\n",
      "For top 10 documents, we have :\n",
      "\n",
      "Queries\t\tPrecision\tRecall\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Query 1 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 2 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 3 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 4 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 5 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 6 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 7 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 8 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 9 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 10 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Average Precision (across all 10 queries):  0.0 \n",
      "\n",
      "Average Recall (across all 10 queries):  0.0 \n",
      "\n",
      "\n",
      "For top 50 documents, we have :\n",
      "\n",
      "Queries\t\tPrecision\tRecall\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Query 1 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 2 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 3 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 4 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 5 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 6 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 7 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 8 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 9 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 10 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Average Precision (across all 10 queries):  0.0 \n",
      "\n",
      "Average Recall (across all 10 queries):  0.0 \n",
      "\n",
      "\n",
      "For top 100 documents, we have :\n",
      "\n",
      "Queries\t\tPrecision\tRecall\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Query 1 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 2 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 3 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 4 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 5 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 6 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 7 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 8 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 9 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 10 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Average Precision (across all 10 queries):  0.0 \n",
      "\n",
      "Average Recall (across all 10 queries):  0.0 \n",
      "\n",
      "\n",
      "For top 500 documents, we have :\n",
      "\n",
      "Queries\t\tPrecision\tRecall\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Query 1 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 2 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 3 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 4 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 5 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 6 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 7 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 8 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 9 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Query 10 \t 0.0 \t\t 0.0 \n",
      "\n",
      "Average Precision (across all 10 queries):  0.0 \n",
      "\n",
      "Average Recall (across all 10 queries):  0.0 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ebc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "93b302ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7539dca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbfe38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce0563b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
